{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies","metadata":{"id":"IeS55bLjQ2Fr"}},{"cell_type":"code","source":"import os\nimport cv2\nimport tensorflow as tf\nimport numpy as np\nfrom typing import List, Tuple\nfrom matplotlib import pyplot as pyplot\nimport matplotlib.pyplot as plt\nimport imageio","metadata":{"id":"046Vdg8_naJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.list_physical_devices('GPU')","metadata":{"id":"Vcx1ZIbJnarf","outputId":"859a9a76-7606-4fe6-b01e-420606224fdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\ntry:\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n  pass","metadata":{"id":"NPHXxrx2nbU3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Data Loading Functions","metadata":{"id":"_QqoKT2doNt3"}},{"cell_type":"code","source":"import gdown","metadata":{"id":"i8zdVsVIoM5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = 'https://drive.google.com/u/1/uc?id=1pV_hK8_tFfNQj95JoIHamvBesMWdpeoz'\noutput = 'data.zip'\ngdown.download(url, output, quiet = False)","metadata":{"id":"uNWExDz6SqwU","outputId":"8a2746bf-9bd6-4d02-e3e1-bc11944cd897"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.extractall('data.zip')","metadata":{"id":"9Q4mpjBSSrCD","outputId":"00accaa9-37e7-484d-ca23-9fb70a1bba0a","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_video(path: str) -> List[float]:\n    \"\"\"\n    Load a video file from the specified path, preprocess its frames, and return a list of standardized frames.\n\n    Parameters:\n    - path (str): The path to the video file to be loaded.\n\n    Returns:\n    - List[float]: A list of preprocessed frames as TensorFlow tensors. Each frame is standardized.\n                   The frames are extracted from the original video, converted to grayscale, cropped,\n                   and then standardized by subtracting the mean and dividing by the standard deviation.\n    \"\"\"\n    # Open the video file for reading\n    cap = cv2.VideoCapture(path)\n\n    # Initialize a list to store the processed frames\n    frames = []\n\n    # Loop through all the frames in the video\n    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n        ret, frame = cap.read()\n\n        # Convert the frame to grayscale\n        frame = tf.image.rgb_to_grayscale(frame)\n\n        # Crop the frame to the desired region of interest\n        frames.append(frame[190:236, 80:220, :])\n\n    # Release the video capture object\n    cap.release()\n\n    # Calculate the mean and standard deviation of the frames\n    mean = tf.math.reduce_mean(frames)\n    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n\n    # Standardize the frames by subtracting the mean and dividing by the standard deviation\n    standardized_frames = tf.cast((frames - mean), tf.float32) / std\n\n    return standardized_frames\n","metadata":{"id":"V7QYmWQpoNIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = [char for char in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]","metadata":{"id":"OdpnDgGz5iw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token = \"\")\nnum_to_char = tf.keras.layers.StringLookup(\n    vocabulary = char_to_num.get_vocabulary(), oov_token = \"\", invert = True\n)\n\nprint(\n    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n    f\"size ={char_to_num.vocabulary_size()}\"\n)","metadata":{"id":"VzDJvoTi5ipc","outputId":"5dca541f-08c9-4100-8758-12f68ca0202c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_alignments(path: str) -> List[str]:\n    \"\"\"\n    Load alignment data from a text file at the specified path and return a list of phonetic tokens.\n\n    Parameters:\n    - path (str): The path to the text file containing alignment data.\n\n    Returns:\n    - List[str]: A list of phonetic tokens extracted from the alignment data.\n    \"\"\"\n    # Open the file at the given path for reading\n    with open(path, 'r') as f:\n        lines = f.readlines()\n\n    # Initialize a list to store the phonetic tokens\n    tokens = []\n\n    # Iterate over each line in the file\n    for line in lines:\n        # Split the line into tokens\n        line = line.split()\n\n        # Check if the third element is not 'sil' (silence)\n        if line[2] != 'sil':\n            # Add the token to the list\n            tokens = [*tokens, ' ', line[2]]\n\n    # Convert the list of tokens to a single string, excluding the first character\n    # and use Unicode split for proper tokenization\n    tokenized_string = char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]\n\n    return tokenized_string\n","metadata":{"id":"cFzv3NnF5ikI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(path: str):\n    \"\"\"\n    Load video frames and corresponding alignments data for a given file path.\n\n    Parameters:\n    - path (str): The path to the video file to be loaded.\n\n    Returns:\n    - Tuple[List[tf.Tensor], List[str]]: A tuple containing two lists:\n        - The first list contains preprocessed video frames as TensorFlow tensors.\n        - The second list contains alignment data as strings.\n\n    Note:\n    - The function expects that the video file is located in the 'data/s1/' directory\n      and follows the naming convention 'filename.mpg'.\n    - The alignment file is expected to be in the 'data/alignments/s1/' directory\n      and follows the naming convention 'filename.align'.\n    \"\"\"\n    # Decode the input path from bytes to string\n    path = bytes.decode(path.numpy())\n\n    # Extract the file name from the path (Windows version)\n    file_name = path.split('/')[-1].split('.')[0]\n\n    # Create full paths for the video and alignment files\n    video_path = os.path.join('data','s1',f'{file_name}.mpg')\n    alignment_path = os.path.join('data','alignments','s1',f'{file_name}.align')\n\n    # Load video frames\n    frames = load_video(video_path)\n\n    # Load alignment data\n    alignments = load_alignments(alignment_path)\n\n    return frames, alignments","metadata":{"id":"6BHNtBBysG0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = './data/s1/bbal6n.mpg'","metadata":{"id":"ax0rQhuqsHM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames, alignments = load_data(tf.convert_to_tensor(test_path))","metadata":{"id":"2Kj-MjNO5ihV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames\n# shape = (75, 46, 140, 1)\n# Meaning:\n# 75: n of frames in lenght (75 different images)\n# 46: pixels high\n# 140: pixels wide\n# 1: one channel, because we've gone and converted that into RGB","metadata":{"id":"3Bhqca6T9RU1","outputId":"00f6b5c3-bac0-4238-a438-8009e7ced637"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames_plt = [frames[0], frames[10], frames[30], frames[60]]  # Replace with your actual frames\n\n# Create a 2x2 grid of subplots\nplt.figure(figsize=(10, 8))  # Adjust the figure size as needed\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)  # Create the i-th subplot in a 2x2 grid\n    plt.imshow(frames_plt[i])     # Display the i-th frame\n\nplt.tight_layout()  # Ensure proper spacing between subplots\nplt.show()  # Show all the subplots in the same cell\n\n# Substracting the mean and variance helps isolating the region, highlighted in yellow-green","metadata":{"id":"1I4YvjeGnKja","outputId":"4ca0d393-748b-4cee-d012-756fcc47884f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alignments\n# This is the numeric representation of what is being said","metadata":{"id":"heXFwkHO9Wgj","outputId":"03bf4c45-f117-4c45-fb8c-7304099c1d1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])","metadata":{"id":"hRAKwje3mwzC","outputId":"87273e3a-db44-4969-c9eb-4ce7a5cabebf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])","metadata":{"id":"SKuuNImYE9zg","outputId":"2b99ef49-6383-4d3b-f0e3-9471e6bf9612"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mappable_function(path:str) ->List[str]:\n    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n    return result","metadata":{"id":"DZlIA7xX5ies"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{"id":"LKZIp_TMqNGL"}},{"cell_type":"code","source":"data = tf.data.Dataset.list_files('./data/s1/*.mpg')\ndata = data.shuffle(500, reshuffle_each_iteration=False)\ndata = data.map(mappable_function)\ndata = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))\ndata = data.prefetch(tf.data.AUTOTUNE)\n# Added for split\ntrain = data.take(450)\ntest = data.skip(450)","metadata":{"id":"7mXt0oGyqRHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames, alignments = data.as_numpy_iterator().next()","metadata":{"id":"1IcvXJJqqRC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames.shape # 2 sets of videos inside each batch","metadata":{"id":"5m1rJHArqQ_w","outputId":"6a182428-8ee1-48a4-f175-2fe5cdd40280"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = data.as_numpy_iterator()","metadata":{"id":"WyZ4-TfwyQ99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = test.next(); val[0]","metadata":{"id":"Eikr3ECu5iba","outputId":"87ceb9ba-89e3-4ea1-83b2-5c8116b1ab7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fv = (val[0][1].astype(np.uint8) * 255).squeeze()\n# imageio.mimsave('./animation.gif', fv, duration=100)\n","metadata":{"id":"YDb7kPnlqQmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0:videos, 0: 1st video out of the batch,  0: return the first frame in the video\n# first 0: videos\n# second 0: 1st video out of the batch\n# third 0: return the first frame in the video\nplt.imshow(val[0][0][0]) # we have 75 frames in each video","metadata":{"id":"TvrdIdAR5iWk","outputId":"6fdccbe9-2bba-427a-bb40-e2e21c76aa2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])","metadata":{"id":"6-RxEikD5Myl","outputId":"be71985a-fa37-4f75-b72c-b8857203a4d7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Design the Deep Neural Network","metadata":{"id":"KTxVw7dN6B8A"}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler","metadata":{"id":"3zY_SI6J5MwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.as_numpy_iterator().next()[0][0].shape","metadata":{"id":"l6YwyQLG5Mts","outputId":"40c48cb8-ebc2-4aa1-9a17-a5331b8c5e36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nNeural Network Model for Video-Based Handwriting Recognition\n\nThis model is designed for recognizing handwriting in video sequences. It consists of convolutional layers for feature extraction, followed by Bidirectional LSTMs for sequence modeling, and a final dense layer for character classification.\n\nArchitecture:\n- Convolutional Layers: These layers extract spatial features from input video frames.\n- Bidirectional LSTMs: These layers model temporal dependencies in the video sequence.\n- Dense Layer: This layer produces character predictions.\n\nParameters:\n- Input Shape: (75, 46, 140, 1) - Input video frame dimensions.\n- Conv3D Layers: Three convolutional layers with varying filter sizes and ReLU activations.\n- MaxPool3D Layers: Max-pooling layers to downsample feature maps.\n- TimeDistributed Layer: Flattens the feature maps.\n- Bidirectional LSTMs: Two Bidirectional LSTM layers with dropout for sequence modeling.\n- Dense Layer: Final output layer with softmax activation for character classification.\n\nNote:\n- The model is designed for character recognition tasks with a vocabulary size of (char_to_num.vocabulary_size() + 1).\n\nAttributes:\n    - model: A Keras Sequential model representing the neural network architecture.\n\nExample Usage:\n    model = Sequential()\n    # Add layers as described above\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\"\"\"\n\n# Create a Sequential model, which is a linear stack of layers.\nmodel = Sequential()\n\n# Add a 3D convolutional layer with 128 filters, a 3x3x3 kernel, and input shape of (75, 46, 140, 1).\n# Padding is used to maintain the same spatial dimensions.\nmodel.add(Conv3D(128, 3, input_shape=(75, 46, 140, 1), padding='same'))\nmodel.add(Activation('relu'))  # Apply ReLU activation function.\nmodel.add(MaxPool3D((1, 2, 2)))  # Apply 3D max-pooling with a (1, 2, 2) pool size.\n\n# Add another 3D convolutional layer with 256 filters and ReLU activation.\n# Padding is 'same' to maintain dimensions.\nmodel.add(Conv3D(256, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1, 2, 2)))\n\n# Add another 3D convolutional layer with 75 filters and ReLU activation.\n# Padding is 'same' to maintain dimensions.\nmodel.add(Conv3D(75, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool3D((1, 2, 2)))\n\n# TimeDistributed layer is used to apply Flatten to each time step separately.\nmodel.add(TimeDistributed(Flatten()))\n\n# Add a Bidirectional LSTM layer with 128 units and 'Orthogonal' kernel initialization.\n# Return sequences to get outputs at each time step.\nmodel.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\nmodel.add(Dropout(.5))  # Apply dropout for regularization.\n\n# Add another Bidirectional LSTM layer with 128 units and 'Orthogonal' kernel initialization.\n# Return sequences to get outputs at each time step.\nmodel.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\nmodel.add(Dropout(.5))  # Apply dropout for regularization.\n\n# Add a Dense layer with output size equal to the vocabulary size plus one.\n# Use 'he_normal' kernel initialization and softmax activation.\nmodel.add(Dense(char_to_num.vocabulary_size() + 1, kernel_initializer='he_normal', activation='softmax'))\n","metadata":{"id":"_Qfcg-3z5Mra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"``a``","metadata":{"id":"kOlcvFk_w20P"}},{"cell_type":"code","source":"model.summary()","metadata":{"id":"Atdc1AgU5Moi","outputId":"30c66701-5e6b-4784-8215-7eaca9be5de0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = model.predict(val[0])","metadata":{"id":"xRR_Gz4QwKeE","outputId":"a583ac61-178f-4f80-84d2-0e09892e19b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[1]])","metadata":{"id":"vSXoR4XPwKbW","outputId":"34b89fbe-7f8a-4414-b50c-0383a00cd6cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat[0].shape\n# we are getting 75 outputs, each one one of these represented as an array with 41 values, which is\n# a one-hot encoding of our vocabulary","metadata":{"id":"rolVtX_WwKY4","outputId":"bbcb9306-240d-4f7f-ea80-e8efaa4f3d06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.argmax(yhat[0], axis=1) # this is returning back what our model is actually predicting","metadata":{"id":"FSoHrudewKWQ","outputId":"dff2fa62-736a-47e8-ccae-36b8c2e7a18d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Training Options and Train","metadata":{"id":"v_V72CDIzVQ0"}},{"cell_type":"code","source":"def scheduler(epoch, lr):\n    if epoch < 30:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)","metadata":{"id":"UOftx_TmwKTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CTCLoss(y_true, y_pred):\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss","metadata":{"id":"tc1HYGF5wKM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProduceExample(tf.keras.callbacks.Callback):\n    def __init__(self, dataset) -> None:\n        self.dataset = dataset\n\n    def on_epoch_end(self, epoch, logs=None) -> None:\n        data = self.dataset.next()\n        yhat = self.model.predict(data[0])\n        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()\n        for x in range(len(yhat)):\n            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n            print('~'*100)","metadata":{"id":"WGE7gsXG5MlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)","metadata":{"id":"vxZCqfx-zksJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(os.path.join('model','checkpoint'), monitor='loss', save_weights_only=True)","metadata":{"id":"alMq6wuYzkkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"schedule_callback = LearningRateScheduler(scheduler)","metadata":{"id":"IQr4u4xSzkhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_callback = ProduceExample(test)","metadata":{"id":"l9TNBBcvzkei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Predictions","metadata":{"id":"w7VkYasYBZDh"}},{"cell_type":"code","source":"# model.fit(train, validation_data=test, epochs=100, callbacks=[checkpoint_callback, schedule_callback, example_callback])","metadata":{"id":"ivjXB0T4zkbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = 'https://drive.google.com/u/1/uc?id=1HzJYaEntb1-jddsZ-254eGj9uTlrqx5v'\noutput = 'checkpoints.zip'\ngdown.download(url, output, quiet = False)","metadata":{"id":"fOuZ6mh6zkXa","outputId":"f752b441-0766-4cfc-df26-99222df03d82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.extractall('checkpoints.zip', 'models')","metadata":{"id":"vMMyJt1Q5iMc","outputId":"c1bf1d3f-47f9-4f94-8b54-a037a661e822"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('models/checkpoint')","metadata":{"id":"80mVLOlNCTxc","outputId":"14870264-54ab-4f08-9c57-1aa3d20a8bd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data = test.as_numpy_iterrator()","metadata":{"id":"VuT37EBOBbS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.next()","metadata":{"id":"4B3V9UHZBbP-","outputId":"ccdf8769-1c16-4901-d7d9-171ae77105e2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = test.next()","metadata":{"id":"vrbRB8E5BbNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = model.predict(sample[0])","metadata":{"id":"b1FqvCaBBbKg","outputId":"4425e59d-66be-4502-a177-36ca8248266e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('~'*100, 'REAL TEXT')\n[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in sample[1]]","metadata":{"id":"WA5XqgvOB1Dm","outputId":"5cb56d4c-8a81-460e-c23b-498026306111"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()","metadata":{"id":"4q3kGLIAB1BH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('~'*100, 'PREDICTIONS')\n[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]","metadata":{"id":"Rg1KS9s5B0-0","outputId":"2e9cd1f8-787c-4a96-d9fc-531413792455"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"fntLQY27B08N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"i4R_KaWZB05k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hDuYyrZrB025"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"LXOqlGqgB00B"},"execution_count":null,"outputs":[]}]}